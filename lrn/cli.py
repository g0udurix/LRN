#!/usr/bin/env python3\nimport argparse, os, sys, re, hashlib, json, subprocess, shutil\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\n\ndef sha256_bytes(data: bytes) -> str:\n    import hashlib\n    h = hashlib.sha256(); h.update(data); return h.hexdigest()\n\ndef write_text(path, text):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, 'w', encoding='utf-8') as f: f.write(text)\n\ndef write_bin(path, data: bytes):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, 'wb') as f: f.write(data)\n\ndef find_inner_xhtml(html: str) -> str:\n    # Heuristic: inner fragment starts at the XML declaration or XHTML DOCTYPE and ends at the closing root div\n    m = re.search(r'(?:<\?xml[^>]*\?>\s*)?<!DOCTYPE\s+div[^>]*>\s*<div\b[\s\S]*</div>\s*', html, re.IGNORECASE)\n    if m: return m.group(0)\n    # Fallback: find card-body container id mainContent-document then capture following XML/DOCTYPE+div\n    m2 = re.search(r'id="mainContent-document"[\s\S]*?(<\?xml[\s\S]*?</div>\s*)', html, re.IGNORECASE)\n    if m2: return m2.group(1)\n    raise RuntimeError('Inner XHTML fragment not found')\n\ndef detect_instrument(html_path: str, frag: BeautifulSoup) -> str:\n    # Try to derive instrument from Identification-Id or filename as fallback\n    ident = frag.find(class_=re.compile(r'Identification-Id'))\n    if ident:\n        txt = ident.get_text(' ', strip=True)\n        # Look for S-2.1, r. 8.2 pattern\n        m = re.search(r'[A-Z]-[0-9]+\.?[0-9]*,\s*r\.\s*[^\s]+', txt)\n        if m:\n            return m.group(0).replace(' ', '').replace(',', '_').replace('.', '_').replace('/', '-')\n    # Fallback to filename stem\n    base = os.path.basename(html_path)\n    return os.path.splitext(base)[0]\n\ndef extract(history_sidecars: bool, history_markdown: bool, annex_pdf_to_md: bool, metadata_exclusion: str, out_dir: str, inputs: list[str], base_url: str|None, pdf_to_md_engine: str, ocr: bool):\n    for src in inputs:\n        with open(src, 'r', encoding='utf-8', errors='ignore') as f:\n            html = f.read()\n        frag_html = find_inner_xhtml(html)\n        frag_soup = BeautifulSoup(frag_html, 'lxml')\n        instrument = detect_instrument(src, frag_soup)\n        inst_dir = os.path.join(out_dir, instrument)\n        # Save intact fragment\n        current_path = os.path.join(inst_dir, 'current.xhtml')\n        write_text(current_path, frag_html)\n        # Inject enrichment links placeholder list container if not present\n        # Annex handling\n        if annex_pdf_to_md:\n            for a in frag_soup.find_all('a', href=True):\n                href = a['href']\n                if href.lower().endswith('.pdf'):\n                    abs_url = urljoin(base_url or '', href) if not bool(urlparse(href).scheme) else href\n                    try:\n                        import requests\n                        r = requests.get(abs_url, timeout=60)\n                        r.raise_for_status()\n                        pdf_bytes = r.content\n                    except Exception as e:\n                        print(f'[WARN] PDF fetch failed {abs_url}: {e}', file=sys.stderr)\n                        continue\n                    pdf_sha = sha256_bytes(pdf_bytes)\n                    pdf_name = os.path.basename(urlparse(abs_url).path) or 'annex.pdf'\n                    pdf_dir = os.path.join(inst_dir, 'annexes')\n                    pdf_path = os.path.join(pdf_dir, pdf_name)\n                    write_bin(pdf_path, pdf_bytes)\n                    md_path = os.path.splitext(pdf_path)[0] + '.md'\n                    # Convert via marker (conda env tool) -> assume executable 'marker'\n                    # We write a temp file and shell out\n                    try:\n                        # Prefer marker direct conversion\n                        subprocess.run(['marker', '--input', pdf_path, '--output', md_path, '--format', 'gfm'], check=True)\n                        # Prepend YAML front matter\n                        fm = f"---\nsource_url: {abs_url}\nsha256: {pdf_sha}\n---\n\n"\n                        with open(md_path, 'r+', encoding='utf-8') as md:\n                            content = md.read(); md.seek(0); md.write(fm + content); md.truncate()\n                        # Inject sibling link in XHTML copy on disk\n                        rel_md = os.path.relpath(md_path, inst_dir).replace(os.sep, '/')\n                        rel_pdf = os.path.relpath(pdf_path, inst_dir).replace(os.sep, '/')\n                        a.insert_after(f' [Version Markdown]({rel_md})')\n                        # Update current.xhtml with injected link later\n                    except Exception as e:\n                        print(f'[WARN] marker conversion failed for {pdf_path}: {e}', file=sys.stderr)\n                        # Leave PDF only\n            # After modifying soup, write enriched XHTML as current.xhtml\n            write_text(current_path, str(frag_soup))\n        # History sidecars (skeleton: capture links present; full crawl to implement later)\n        if history_sidecars:\n            from collections import defaultdict\n            items = []\n            for img in frag_soup.find_all('img', src=True):\n                if 'history' in img['src']:\n                    a = img.find_parent('a')\n                    if a and a.get('href'):\n                        items.append(a['href'])\n            if items:\n                idx = {\n                    'fragment_links': items,\n                }\n                write_text(os.path.join(inst_dir, 'history', 'index.json'), json.dumps(idx, ensure_ascii=False, indent=2))\n\nif __name__ == '__main__':\n    p = argparse.ArgumentParser(description='LRN extractor for LegisQuébec HTML -> inner XHTML, with annex and history enrichment')\n    p.add_argument('inputs', nargs='+', help='Input HTML files')\n    p.add_argument('--out-dir', default='output', help='Output directory')\n    p.add_argument('--base-url', default='', help='Base URL to resolve relative links')\n    p.add_argument('--annex-pdf-to-md', action='store_true', default=True, help='Convert annex PDFs to Markdown using marker')\n    p.add_argument('--history-sidecars', action='store_true', default=True, help='Capture history links (sidecars crawl placeholder)')\n    p.add_argument('--history-markdown', action='store_true', default=True, help='Also emit Markdown for history snapshots (future)')\n    p.add_argument('--metadata-exclusion', default='', help='Metadata exclusion profile (kept empty to keep-all)')\n    p.add_argument('--pdf-to-md-engine', default='marker', help='Engine for PDF→MD (marker)')\n    p.add_argument('--ocr', action='store_true', default=False, help='Enable OCR fallback (future)')\n    args = p.parse_args()\n    extract(args.history_sidecars, args.history_markdown, args.annex_pdf_to_md, args.metadata_exclusion, args.out_dir, args.inputs, args.base_url or None, args.pdf_to_md_engine, args.ocr)\n
